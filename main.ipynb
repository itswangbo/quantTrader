{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from qids_package.qids import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import datetime\n",
    "import scipy\n",
    "\n",
    "import random\n",
    "warnings.simplefilter('ignore')\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class Transformer_ours(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_features_encoder: int, # num of features in the encoder,\n",
    "        in_features_decoder: int, # num of features in the decoder\n",
    "        out_features: int, # num of features to represent the predicted stock information, default value will be: 1\n",
    "        enc_seq_len: int,\n",
    "        dec_seq_len: int,\n",
    "        d_model: int,  \n",
    "        n_encoder_layers: int,\n",
    "        n_decoder_layers: int,\n",
    "        n_heads: int,\n",
    "        dropout_encoder: float=0.2, \n",
    "        dropout_decoder: float=0.2,\n",
    "        dropout_pos_enc: float=0.2,\n",
    "        dropout_pos_dec: float=0.2,\n",
    "        dim_feedforward_encoder: int=2048,\n",
    "        dim_feedforward_decoder: int=2048,\n",
    "        conv_size: int = 10,\n",
    "        long_dependency_multiplier: int = 4\n",
    "        ): \n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.enc_seq_len = enc_seq_len\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "        self.long_dependency_multiplier = long_dependency_multiplier\n",
    "        self.in_features_encoder = in_features_encoder\n",
    "        self.in_features_decoder = in_features_decoder\n",
    "        # after adding long dependency, #cols becomes 1.5*in_features\n",
    "        self.encoder_input_layer = nn.Linear(in_features=int(in_features_encoder*1.5), out_features=d_model)\n",
    "        self.decoder_input_layer = nn.Linear(in_features=in_features_decoder, out_features=d_model)  \n",
    "        \n",
    "        self.linear_mapping = nn.Linear(in_features=d_model,out_features=out_features)\n",
    "        self.long_dependency_compression = nn.Linear(in_features = int(in_features_encoder*1.5), out_features = int(in_features_encoder*0.5))\n",
    "        \n",
    "        # self.positional_encoding_layer = Time2Vector(seq_len=enc_seq_len, out_features=d_model, dropout=dropout_pos_enc)\n",
    "        # self.positional_decoding_layer = Time2Vector(seq_len=dec_seq_len, out_features=d_model, dropout=dropout_pos_dec)\n",
    "        \n",
    "        encoder_layer = MyEncoder(\n",
    "            d_model=d_model, \n",
    "            nheads=n_heads,\n",
    "            dim_feedforward_encoder=dim_feedforward_encoder,\n",
    "            dropout_encoder=dropout_encoder,\n",
    "            conv_size=conv_size,\n",
    "            )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer,num_layers=n_encoder_layers, norm=None)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_decoder,\n",
    "            dropout=dropout_decoder,\n",
    "            batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer,num_layers=n_decoder_layers, norm=None)\n",
    "    \n",
    "    def forward(self, src, tgt):    \n",
    "        encoder_seq_len_true = int(self.enc_seq_len/self.long_dependency_multiplier)\n",
    "        '''add long dependency'''\n",
    "        new_src = torch.empty(len(src), encoder_seq_len_true, int(1.5*self.in_features_encoder))\n",
    "        history = torch.zeros(encoder_seq_len_true, int(self.in_features_encoder*0.5))\n",
    "        for i in range(len(src)):\n",
    "            for j in range(self.long_dependency_multiplier):\n",
    "                src_temp = src[i][j*encoder_seq_len_true: (j+1)*encoder_seq_len_true]\n",
    "                src_temp = torch.concat([src_temp, history], dim = -1)\n",
    "                history = self.long_dependency_compression(src_temp)\n",
    "            new_src[i] = src_temp      \n",
    "        new_src = self.encoder_input_layer(new_src)\n",
    "        new_src = self.encoder(src=new_src)      \n",
    "        tgt = self.decoder_input_layer(tgt)\n",
    "        tgt = self.decoder(tgt=tgt, memory=new_src)\n",
    "\n",
    "        decoder_output= self.linear_mapping(tgt)\n",
    "\n",
    "        return decoder_output.squeeze()\n",
    "\n",
    "\n",
    "class MyEncoder(nn.Module):\n",
    "    def __init__(self, d_model: int, nheads: int, dim_feedforward_encoder: int = 2048, dropout_encoder: float = 0.2, conv_size: int = 10):\n",
    "        super(MyEncoder, self).__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nheads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=True)\n",
    "\n",
    "        self.convolution_layer = nn.Conv2d(1,1,(conv_size,1),stride = 1,bias = False)\n",
    "        self.combine_encoder_conv = nn.Linear(in_features = 2*d_model, out_features = d_model)\n",
    "        self.conv_size = conv_size\n",
    "\n",
    "    def forward(self, x, src_mask: Tensor= None, src_key_padding_mask: Tensor = None): \n",
    "        encoder_layer_temp = self.encoder_layer(src = x)   \n",
    "        x_pad = F.pad(x, (0,0, self.conv_size - 1,0)) # add padding so that the output of a convolution layer and encoder layer are the same\n",
    "        x_pad = torch.unsqueeze(x_pad, dim = 1)\n",
    "        conv_layer_temp = self.convolution_layer(x_pad)\n",
    "        conv_layer_temp = torch.squeeze(conv_layer_temp, dim = 1) \n",
    "        temp = torch.concat([encoder_layer_temp, conv_layer_temp], dim = -1)\n",
    "        x = self.combine_encoder_conv(temp)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import datetime\n",
    "import scipy\n",
    "from sklearn import metrics\n",
    "\n",
    "path_fund = './first_round_train_fundamental_data.csv'\n",
    "path_market = './first_round_train_market_data.csv'\n",
    "path_ret = './first_round_train_return_data.csv'\n",
    "\n",
    "def get_data(path_fund, path_market, path_ret):\n",
    "    train_fund_data = pd.read_csv(path_fund)\n",
    "    train_market_data = pd.read_csv(path_market)\n",
    "    train_return_data = pd.read_csv(path_ret)\n",
    "    return train_fund_data, train_market_data, train_return_data\n",
    "\n",
    "def data_process(_data):\n",
    "    data = {}\n",
    "    _data[['code','time']] = _data['date_time'].str.split(\"d\", expand=True)\n",
    "    _data = _data.drop(['date_time','time'], axis = 1)\n",
    "    \n",
    "    _data = _data.groupby('code')\n",
    "    for name, group in _data:\n",
    "        group = group.drop('code', axis=1).to_numpy().astype(float)\n",
    "        group = torch.FloatTensor(group)\n",
    "        data[name] = group\n",
    "    return data\n",
    "\n",
    "def all_stock_train_eval(data_fund, data_market, data_ret,\n",
    "                         batch_size : int = 64, \n",
    "                         sample_rate: float = 0.5,\n",
    "                         enc_seq_len: int = 5*50,\n",
    "                         dec_seq_len: int = 5):\n",
    "    total_loss = 0.\n",
    "\n",
    "    rand = np.array([0, 1])\n",
    "    prob = np.array([1-sample_rate, sample_rate])\n",
    "\n",
    "    train_seq, test_seq = [], []\n",
    "    \n",
    "    for key in data_fund.keys():\n",
    "        # number of records of stock available for training\n",
    "        data_range = len(data_fund[key])-dec_seq_len - 2\n",
    "        for i in range(0, data_range, 1):\n",
    "            # print(data_market[key][i*enc_seq_len:(i+1)*enc_seq_len, :].shape)\n",
    "            if random.randint(0, 3) == 0:  # 1/4 test; 3/4 train\n",
    "                test_seq.append((data_market[key][(i+dec_seq_len)*50-enc_seq_len:(i+dec_seq_len)*50, :], data_fund[key][i:i+dec_seq_len, :], data_ret[key][i+dec_seq_len-1:i+dec_seq_len, -1]))\n",
    "            else:\n",
    "                train_seq.append((data_market[key][(i+dec_seq_len)*50-enc_seq_len:(i+dec_seq_len)*50, :], data_fund[key][i:i+dec_seq_len, :], data_ret[key][i+dec_seq_len-1:i+dec_seq_len, -1]))\n",
    "\n",
    "    random.shuffle(train_seq) # shuffle all stocks train seq\n",
    "    random.shuffle(test_seq)  # shuffle all stocks test seq\n",
    "    \n",
    "    train_src_seq, train_tgt_seq, train_tgt_y_seq = [], [], []\n",
    "    test_src_seq, test_tgt_seq, test_tgt_y_seq = [], [], []\n",
    "\n",
    "    for seq in train_seq:\n",
    "        if np.random.choice(a=rand, size=1, replace=True, p=prob) == 1: # with probability sample_rate, keep the training seq, otherwise discard\n",
    "            train_src_seq.append(seq[0])\n",
    "            train_tgt_seq.append(seq[1])\n",
    "            train_tgt_y_seq.append(seq[2])\n",
    "\n",
    "    for seq in test_seq:\n",
    "        test_src_seq.append(seq[0])\n",
    "        test_tgt_seq.append(seq[1])\n",
    "        test_tgt_y_seq.append(seq[2])\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_src_seq), batch_size)):\n",
    "        data_len = min(batch_size, len(train_src_seq)-batch_size*batch)\n",
    "        optimizer.zero_grad()\n",
    "        src=torch.stack(train_src_seq[i:i+data_len])\n",
    "        tgt=torch.stack(train_tgt_seq[i:i+data_len])\n",
    "        output = model(src=torch.stack(train_src_seq[i:i+data_len]), tgt=torch.stack(train_tgt_seq[i:i+data_len]))\n",
    "        \n",
    "        loss = criterion(output, torch.stack(train_tgt_y_seq[i:i+data_len]))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        mean_loss = 0.0\n",
    "        for batch, i in enumerate(range(0, len(test_src_seq), batch_size)):\n",
    "            data_len = min(batch_size, len(test_src_seq)-batch_size*batch)\n",
    "            output = model(src=torch.stack(test_src_seq[i:i+data_len]), tgt=torch.stack(test_tgt_seq[i:i+data_len]))\n",
    "            loss = criterion(output, torch.stack(test_tgt_y_seq[i:i+data_len]))\n",
    "            mean_loss = (i*mean_loss+data_len*loss)/(i+data_len)\n",
    "    \n",
    "    # print(\"finish calculate loss of test seq: \")\n",
    "    # print(datetime.datetime.now())\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "\n",
    "d_model = 256\n",
    "n_heads = 8 \n",
    "n_encoder_layers = 2 \n",
    "n_decoder_layers = 2 \n",
    "in_features_encoder = 6 # market data\n",
    "in_features_decoder = 7 # fund data\n",
    "out_features = 1\n",
    "long_dependency_multiplier = 10 # compress 150 to 15 (enc_seq_len)\n",
    "enc_seq_len = 3*50 # 3*50 bar market data\n",
    "dec_seq_len = 3 # 3 days fund data\n",
    "sample_ratio = 0.01\n",
    "conv_size = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('start')\n",
    "\n",
    "train_fund_data, train_market_data, train_ret_data = get_data(path_fund, path_market, path_ret)\n",
    "train_fund_data = data_process(train_fund_data)\n",
    "train_market_data = data_process(train_market_data)\n",
    "train_ret_data = data_process(train_ret_data)\n",
    "\n",
    "print('data processed.')\n",
    "model = Transformer_ours(\n",
    "        d_model=d_model,\n",
    "        in_features_encoder=in_features_encoder,\n",
    "        in_features_decoder=in_features_decoder,\n",
    "        out_features=out_features,\n",
    "        enc_seq_len=enc_seq_len,\n",
    "        dec_seq_len=dec_seq_len,\n",
    "        n_encoder_layers=n_encoder_layers,\n",
    "        n_decoder_layers=n_decoder_layers,\n",
    "        n_heads=n_heads,\n",
    "        conv_size = conv_size,\n",
    "        long_dependency_multiplier=long_dependency_multiplier\n",
    "        )\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "criterion_mae = nn.L1Loss()\n",
    "step_num = 1\n",
    "warmup_step = 5000\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9,0.98), eps=1e-08, lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_num, gamma=0.9)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 50 # edit later    \n",
    "best_loss = float('inf')   \n",
    "stop_count = 0    \n",
    "save_every_epoch = 50\n",
    "\n",
    "    \n",
    "print(\"start training: \")\n",
    "for epoch in range(0, epochs):   \n",
    "    mean_loss = all_stock_train_eval(train_fund_data, train_market_data, train_ret_data, batch_size, sample_rate=sample_ratio)\n",
    "    print('-' * 15)\n",
    "    print('　　　current epoch: {%d}' % epoch)\n",
    "    print('　　　current_loss: {:5.5f}'.format(mean_loss))\n",
    "    print(datetime.datetime.now())\n",
    "    print('-' * 15)\n",
    "    if mean_loss < best_loss:\n",
    "        best_loss = mean_loss\n",
    "        stop_count = 0\n",
    "    else:\n",
    "        stop_count += 1\n",
    "    # if stop_count == 4: #Early stop if no updates occur 4 times\n",
    "    #     break\n",
    "    scheduler.step()  \n",
    "\n",
    "    # save models\n",
    "    if(epoch % save_every_epoch == save_every_epoch - 1):\n",
    "        path = 'model.pt'\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "print('-' * 15)\n",
    "print('　　　best_loss: {:5.5f}'.format(best_loss))\n",
    "print('-' * 15)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment is initialized.\n"
     ]
    }
   ],
   "source": [
    "env = make_env()  # initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code using random number as prediction\n",
    "import random\n",
    "random.seed(42)\n",
    "while not env.is_end():\n",
    "    fundamental_df, market_df = env.get_current_market()  # get correlated dataset\n",
    "\n",
    "    # make your prediction Y here and replace the following four rows\n",
    "    ###### load model here and make prediction\n",
    "    # torch.load(\"model.pt\")\n",
    "    l = []\n",
    "    for idx in range(54):\n",
    "        l.append(random.random())\n",
    "    predict_ds = pd.Series(l)\n",
    "\n",
    "    env.input_prediction(predict_ds)  # upload your predicted Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
